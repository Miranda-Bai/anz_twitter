{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import stanza "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel(\"data/sentiment.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-10 12:14:12 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n",
      "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.5.0.json: 216kB [00:00, 43.2MB/s]                    \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pos/combined.pt: 100%|██████████| 38.5M/38.5M [00:17<00:00, 2.22MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/lemma/combined.pt: 100%|██████████| 4.17M/4.17M [00:01<00:00, 2.61MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/constituency/wsj.pt: 100%|██████████| 113M/113M [00:47<00:00, 2.37MB/s] \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/depparse/combined.pt: 100%|██████████| 145M/145M [00:57<00:00, 2.50MB/s] \n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/ner/ontonotes.pt: 100%|██████████| 46.2M/46.2M [00:07<00:00, 6.03MB/s]\n",
      "Downloading https://huggingface.co/stanfordnlp/stanza-en/resolve/v1.5.0/models/pretrain/fasttextcrawl.pt: 100%|██████████| 123M/123M [00:19<00:00, 6.41MB/s] \n",
      "2023-09-10 12:17:02 INFO: Loading these models for language: en (English):\n",
      "============================\n",
      "| Processor    | Package   |\n",
      "----------------------------\n",
      "| tokenize     | combined  |\n",
      "| pos          | combined  |\n",
      "| lemma        | combined  |\n",
      "| constituency | wsj       |\n",
      "| depparse     | combined  |\n",
      "| sentiment    | sstplus   |\n",
      "| ner          | ontonotes |\n",
      "============================\n",
      "\n",
      "2023-09-10 12:17:02 INFO: Using device: cpu\n",
      "2023-09-10 12:17:02 INFO: Loading: tokenize\n",
      "2023-09-10 12:17:02 INFO: Loading: pos\n",
      "2023-09-10 12:17:02 INFO: Loading: lemma\n",
      "2023-09-10 12:17:02 INFO: Loading: constituency\n",
      "2023-09-10 12:17:02 INFO: Loading: depparse\n",
      "2023-09-10 12:17:02 INFO: Loading: sentiment\n",
      "2023-09-10 12:17:03 INFO: Loading: ner\n",
      "2023-09-10 12:17:03 INFO: Done loading processors!\n"
     ]
    }
   ],
   "source": [
    "# Load the English model\n",
    "nlp = stanza.Pipeline('en')\n",
    "# Perform frequency analysis\n",
    "word_frequency = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Function to remove URLs using regular expression\n",
    "def remove_urls(text):\n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    return url_pattern.sub('', text)\n",
    "\n",
    "def frequency_analysis(text):\n",
    "    # Process the text\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Define words and characters to exclude\n",
    "    exclude_words = {'anz', 'anzbank'}  # Add any other words to exclude\n",
    "    exclude_characters = {',', '.', '#'}\n",
    "\n",
    "    for sentence in doc.sentences:\n",
    "        for word in sentence.words:\n",
    "            word_text = word.text.lower()  # Convert to lowercase\n",
    "            # Remove URLs from the word text\n",
    "            word_text = remove_urls(word_text)\n",
    "            \n",
    "            # Check if the word should be excluded\n",
    "            if word_text not in exclude_words and word_text not in exclude_characters:\n",
    "                if word_text not in word_frequency:\n",
    "                    word_frequency[word_text] = 1\n",
    "                else:\n",
    "                    word_frequency[word_text] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       None\n",
       "1       None\n",
       "2       None\n",
       "3       None\n",
       "4       None\n",
       "        ... \n",
       "6340    None\n",
       "6341    None\n",
       "6342    None\n",
       "6343    None\n",
       "6344    None\n",
       "Name: raw_content, Length: 6345, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply frequency analysis to the \"raw_content\" column and store results in \"word_frequency\"\n",
    "df['raw_content'].apply(frequency_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort words by frequency (most frequent first)\n",
    "sorted_word_frequency = dict(sorted(word_frequency.items(), key=lambda item: item[1], reverse=True))\n",
    "\n",
    "# Print the most frequent words and their frequencies\n",
    "# num_top_keywords = 10  # Number of top keywords to display\n",
    "# top_keywords = list(sorted_word_frequency.keys())[:num_top_keywords]\n",
    "keyworddf=[]\n",
    "keywords = list(sorted_word_frequency.keys())\n",
    "for keyword in keywords:\n",
    "    keyworddf.append([keyword, sorted_word_frequency[keyword]])\n",
    "    # print(f\"Keyword: {keyword}, Frequency: {sorted_word_frequency[keyword]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_labels = ['Keyword', 'Frequency']\n",
    "\n",
    "pd.DataFrame(keyworddf, columns=index_labels).to_excel(\"data/frequency_analysis.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
